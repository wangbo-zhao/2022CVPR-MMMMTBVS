# 2022CVPR-Modeling-Motion-with-Multi-Modal-Features-for-Text-Based-Video-Segmentation
This is the code for CVPR2022 paper "Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation"

ðŸ”¥ðŸ”¥ðŸ”¥Coming SoonðŸ”¥ðŸ”¥ðŸ”¥
## Usage
1. Download [A2D-Sentences](https://kgavrilyuk.github.io/publication/actor_action/) and [JHMDB-Sentences](https://kgavrilyuk.github.io/publication/actor_action/).

2. Please use [RAFT](https://github.com/princeton-vl/RAFT) to generate the opticla flow map for each frame.

3. Put them as follows:
```
your dataset dir/
â””â”€â”€ A2D/ 
    â”œâ”€â”€ allframes/  
    â”œâ”€â”€ allframes_flow/
    â”œâ”€â”€ Annotations_visualize
    â”œâ”€â”€ a2d_txt
        â””â”€â”€train.txt
        â””â”€â”€test.txt
â””â”€â”€ J-HMDB/ 
    â”œâ”€â”€ allframes/  
    â”œâ”€â”€ allframes_flow/
    â”œâ”€â”€ Annotations_visualize
    â”œâ”€â”€ jhmdb_txt
        â””â”€â”€train.txt
        â””â”€â”€test.txt
```
"Annotations_visualize" contains the GT masks for each target object. We have upload them to [BaiduPan]() for convenience.

## Train

## Inference
 
 
## Citation
Please consider citing our work in your publications if you are interest in our research:
```
wait
```
